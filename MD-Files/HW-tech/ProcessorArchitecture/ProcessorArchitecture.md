# Hardware - Cheatsheet - Processor Architecture

## What is a Processor
A Processor is one of the critical components in a computer system. A Processor also referred to as a central processing unit (CPU), is responsible for executing instructions and performing calculations in a computer system. The Processor acts as the brain of the computer and communicates with other components, such as memory and storage, to ensure that the computer functions properly.

The Processor's primary function is to fetch instructions from memory, decode them, and execute them. The Processor is responsible for performing arithmetic and logical operations, controlling input and output operations, and managing the flow of data between different components of the computer system.

Processors come in different types and models, each with its own set of features and capabilities. Some of the most popular Processor manufacturers include Intel and AMD. With advancements in technology, processors have become faster, more powerful, and more energy-efficient, making it possible for computers to perform complex tasks at lightning-fast speeds.

## Speed/Performance of a Processor
The Processor's speed is measured in gigahertz (GHz), and a higher clock speed generally indicates a faster Processor. However, other factors, such as the number of cores and cache size, also affect the Processor's performance.

## Clock
As you already know the Speed/Performance of a Processor/CPU gets measured in gigahertz also written as GHz. A higher clock speed generally indicated a faster processor. Now you have heard a important term `clock`. Now what does a `clock` or rather what is a `clock`. 

The clock in a CPU architecture is a crucial component that regulates the timing of operations and synchronizes different parts of the CPU. The clock signal is generated by a quartz crystal oscillator and sets the speed at which the CPU can process data. The clock speed is measured in Hertz (Hz) and represents the number of clock cycles per second.

Higher clock speeds generally lead to faster processing times, as the CPU can execute more instructions per second. However, higher clock speeds also generate more heat, which can be a concern in systems with limited cooling capabilities. Additionally, as clock speeds increase, the power consumption of the CPU also increases, which can impact battery life in mobile devices.

In addition to clock speed, other factors can impact the performance of a CPU, such as the number of cores, cache size, and instruction set architecture. Different applications may also have varying requirements for CPU performance, such as video editing or gaming, which may benefit from higher clock speeds or additional cores.

Over the years, CPU clock speeds have increased dramatically, with the first commercial CPUs in the 1970s operating at speeds of just a few kilohertz (kHz). By the 1990s, clock speeds had increased to several hundred megahertz (MHz), and by the early 2000s, speeds had surpassed 1 gigahertz (GHz). Today, some CPUs have clock speeds in excess of 5 GHz.

Despite these increases in clock speed, CPU manufacturers have also sought to optimize performance in other ways, such as through pipelining, parallel processing, and improved cache organization. Additionally, recent trends in CPU design have focused on increasing energy efficiency, reducing power consumption, and optimizing performance for specific workloads, such as machine learning or data analytics.

## ISA - Instruction Set Architecture
Instruction Set Architecture (ISA) refers to the set of instructions that a CPU architecture can execute. The ISA defines the available instructions that a programmer can use to write software that runs on the CPU.

The ISA includes instructions for basic arithmetic and logical operations, memory access, branching and control flow, and other operations. The number and types of instructions available in the ISA can vary widely between CPU architectures. For example, some architectures may have specialized instructions for graphics processing or machine learning, while others may not.

The ISA is important because it defines the interface between the software and the hardware, and affects the performance and capabilities of the CPU. Different ISAs may have different levels of support for parallel processing, which can impact the ability of the CPU to execute multiple instructions simultaneously.

Additionally, different ISAs may have different levels of compatibility with existing software, which can impact the adoption of new CPU architectures. For example, software written for one ISA may not be compatible with a CPU that uses a different ISA, without additional software emulation or translation.

There are several different types of ISAs, including complex instruction set computing (CISC) and reduced instruction set computing (RISC). CISC architectures generally include a large number of instructions that can perform complex operations with fewer instructions, while RISC architectures typically have a smaller set of simpler instructions that can be executed more quickly.

In recent years, there has been a trend towards creating specialized ISAs for specific workloads, such as machine learning or data analytics. These specialized ISAs can optimize performance for specific tasks by providing instructions tailored to those workloads. Additionally, some modern CPUs support multiple ISAs, allowing them to execute software written for different architectures.

## Cache

In computer architecture, cache is a type of high-speed memory that stores frequently accessed data or instructions, with the goal of improving system performance. A CPU cache can significantly reduce the amount of time required to access data from main memory, which is typically much slower.

The cache memory is usually organized in a hierarchy, with different levels of cache providing increasingly larger storage capacity and slower access times. The smallest and fastest cache is called the L1 cache and is usually located on the same chip as the CPU. L2 and L3 caches are typically located on separate chips and offer higher storage capacity but slower access times.

The cache operates by storing a subset of the data from main memory that is expected to be accessed in the near future. When the CPU needs to access data, it first checks the cache to see if the data is available. If the data is found in the cache (a cache hit), the CPU can access it much faster than if it had to retrieve it from main memory (a cache miss).

Cache management is an important aspect of CPU architecture, as it directly impacts system performance. The cache is typically managed by hardware, which uses algorithms to determine which data to store in the cache and when to evict data to make room for new data. Additionally, software can be optimized to take advantage of the cache by ensuring that frequently accessed data is stored in the cache and reducing the frequency of cache misses.

Different CPU architectures may use different types of caches, such as write-through or write-back caches, or use different cache replacement policies, such as least recently used (LRU) or random replacement. The size and organization of the cache can also vary depending on the CPU architecture, with some CPUs using multiple levels of cache or non-uniform memory access (NUMA) architectures to improve cache performance.

## Effect of different CPU Architecture on performance

CPU architectures play a significant role in the performance and energy efficiency of mobile devices, as these devices have limited power and thermal budgets compared to desktop or server CPUs. Here are a few ways in which CPU architectures can impact the performance and energy efficiency of mobile devices:

1. Instruction Set Architecture (ISA): Different ISAs can have a significant impact on performance and energy efficiency. For example, a RISC-based ISA typically requires fewer transistors to implement an instruction, which can lead to a smaller chip size and lower power consumption. On the other hand, a CISC-based ISA can allow for more complex instructions that reduce the number of instructions needed to perform a given task, potentially leading to higher performance.

2. Microarchitecture: The microarchitecture of a CPU determines how the instructions in the ISA are executed, including the pipeline depth, cache hierarchy, and branch predictor. These factors can have a significant impact on performance and energy efficiency. For example, a deeper pipeline can increase performance by allowing for higher clock speeds, but can also increase power consumption due to increased pipeline stalls and instruction fetches.

3. Process Technology: The process technology used to manufacture a CPU can impact both performance and power consumption. Finer process nodes generally allow for higher clock speeds and lower power consumption, as the transistors are smaller and require less power to switch. However, finer process nodes can also be more expensive and difficult to manufacture, and can lead to increased leakage current.

4. Thermal Design: Mobile devices have limited thermal budgets, which can impact the performance of the CPU. Some CPU architectures are more thermally efficient than others, meaning they can achieve higher performance with the same amount of power consumption or operate at lower power levels for the same level of performance. Thermal management techniques, such as dynamic frequency scaling, can also be used to manage the thermal load and improve energy efficiency.

In summary, CPU architectures impact the performance and energy efficiency of mobile devices in several ways. By optimizing for power consumption, transistor count, process technology, and thermal design, mobile CPUs can achieve high levels of performance while still meeting the power and thermal constraints of the device.

## Benefits of Multicore and Singlecore CPU's

A multi-core CPU contains multiple processing units or "cores" on a single chip, while a single-core CPU has only one core. Here are some of the benefits of multi-core CPUs and how they differ from single-core CPUs:

1. Increased Performance: Multi-core CPUs can perform multiple tasks simultaneously, leading to increased performance compared to single-core CPUs. Each core can execute instructions independently, allowing for parallel processing and faster task completion.

2. Better Resource Utilization: Multi-core CPUs can better utilize system resources by distributing tasks across multiple cores. This can improve overall system responsiveness and reduce bottlenecks.

3. Improved Multitasking: Multi-core CPUs are well-suited for multitasking, as they can handle multiple tasks concurrently without significant performance degradation. This can be especially beneficial for users who frequently run multiple applications at once.

4. Energy Efficiency: Multi-core CPUs can be more energy-efficient than single-core CPUs, as they can distribute tasks across multiple cores and reduce the overall workload on each core. This can lead to lower power consumption and longer battery life in mobile devices.

5. Scalability: Multi-core CPUs can be more scalable than single-core CPUs, as additional cores can be added to increase performance as needed. This makes them well-suited for high-performance computing applications that require large amounts of processing power.

However, it's worth noting that not all applications are optimized for multi-core CPUs. Some applications are designed to run on a single core, and may not benefit significantly from additional cores. Additionally, the performance benefits of multi-core CPUs may be limited by the amount of available memory bandwidth, as multiple cores accessing memory simultaneously can lead to contention and reduced performance. Nonetheless, overall, multi-core CPUs offer significant benefits over single-core CPUs in terms of performance, resource utilization, multitasking, energy efficiency, and scalability.

## Old/New - Optional Knowledge about CPU

The terms "Northbridge" and "Southbridge" refer to two important components that were commonly found on older motherboards that used the Intel architecture. The Northbridge and Southbridge were responsible for connecting the CPU to other parts of the computer, such as memory, peripherals, and storage.

The Northbridge was located on the motherboard close to the CPU, and was responsible for handling high-speed data transfers between the CPU and memory, as well as providing support for high-speed expansion slots like PCI Express. The Northbridge also controlled the system bus, which is the pathway that data travels between the CPU and other components.

The Southbridge, on the other hand, was located further away from the CPU and was responsible for providing support for lower-speed peripherals like USB, audio, and network adapters. The Southbridge also handled the slower-speed I/O bus, which connects the CPU to devices like hard drives and optical drives.

On older motherboards, the Northbridge and Southbridge were separate components that communicated with each other using a proprietary interface called the "Front Side Bus" (FSB). The FSB was a data pathway that connected the Northbridge and CPU with the Southbridge and other peripherals.

However, with the introduction of newer CPU architectures and motherboards, the Northbridge and Southbridge have largely been replaced by a single chip called the "Platform Controller Hub" (PCH). The PCH combines the functions of the Northbridge and Southbridge into a single chip, and uses a faster interface called "Direct Media Interface" (DMI) to connect with the CPU.

Overall, the Northbridge and Southbridge were important components of older motherboards that helped to connect the CPU to other parts of the computer. While these components are no longer used in modern motherboards, their legacy can still be seen in the architecture of newer chipsets like the PCH.